# Project Structure and Content for: /Users/ted/Documents/Studying/LLM/Projects/LangGraphLearning/rag_chatbot_project
# Output generated to: /Users/ted/Documents/Studying/LLM/Projects/LangGraphLearning/rag_chatbot_project/project_snapshot.txt
================================================================================

[rag_chatbot_project/]
    get_project.py
        --- START FILE CONTENT ---
        import os
        import fnmatch
        import logging
        from typing import List, Optional
        
        # Configure basic logging for errors during file processing
        logging.basicConfig(
            level=logging.WARNING, format="%(asctime)s - %(levelname)s - %(message)s"
        )
        
        
        def write_project_to_txt(
            project_dir: str,
            output_file: str,
            exclude_patterns: Optional[List[str]] = None,
            include_patterns: Optional[List[str]] = None,
            max_file_size_kb: Optional[int] = None,
            log_skipped: bool = True,
        ):
            if exclude_patterns is None:
                # Default common exclusions
                exclude_patterns = [
                    ".git*",
                    "__pycache__",
                    "*.pyc",
                    "*.pyo",
                    "*.log",
                    ".env",
                    "venv",
                    ".venv",
                    "node_modules",
                    "dist",
                    "build",
                    "*.egg-info",
                    ".DS_Store",
                    ".idea",
                    ".vscode",
                    os.path.basename(output_file),
                ]
                logging.info(f"Using default exclude patterns: {exclude_patterns}")
        
            # Get the absolute path of the output file to ensure accurate exclusion
            abs_output_file = os.path.abspath(output_file)
        
            if not os.path.isdir(project_dir):
                logging.error(
                    f"Error: Project directory '{project_dir}' not found or is not a directory."
                )
                return
        
            try:
                with open(output_file, "w", encoding="utf-8", errors="ignore") as outfile:
                    # Write header
                    outfile.write(
                        f"# Project Structure and Content for: {os.path.abspath(project_dir)}\n"
                    )
                    outfile.write(f"# Output generated to: {abs_output_file}\n")
                    outfile.write("=" * 80 + "\n\n")
        
                    for root, dirs, files in os.walk(project_dir, topdown=True):
                        # --- Exclusion/Inclusion Logic ---
                        # Filter directories based on patterns
                        original_dirs = dirs[:]  # Copy list before modifying
                        dirs[:] = [
                            d
                            for d in dirs
                            if not any(
                                fnmatch.fnmatch(d, pattern) for pattern in exclude_patterns
                            )
                            and (
                                include_patterns is None
                                or any(
                                    fnmatch.fnmatch(os.path.join(root, d), pattern)
                                    for pattern in include_patterns
                                )
                                or any(
                                    fnmatch.fnmatch(d, pattern) for pattern in include_patterns
                                )
                            )  # Check dir name too
                        ]
                        # Log skipped directories
                        if log_skipped:
                            skipped_dirs = set(original_dirs) - set(dirs)
                            for skipped_d in skipped_dirs:
                                logging.info(
                                    f"Skipping directory: {os.path.join(root, skipped_d)}"
                                )
        
                        # Calculate depth for indentation
                        relative_path = os.path.relpath(root, project_dir)
                        depth = relative_path.count(os.sep)
                        indent = "    " * depth  # 4 spaces per level
        
                        # Write current directory path
                        if relative_path == ".":
                            outfile.write(f"[{os.path.basename(project_dir)}/]\n")
                        else:
                            outfile.write(f"{indent}[{os.path.basename(root)}/]\n")
        
                        # Process files in the current directory
                        sub_indent = "    " * (depth + 1)
                        for filename in sorted(files):
                            file_path = os.path.join(root, filename)
                            abs_file_path = os.path.abspath(file_path)
        
                            # Check exclusion patterns for files
                            if any(
                                fnmatch.fnmatch(filename, pattern)
                                for pattern in exclude_patterns
                            ) or any(
                                fnmatch.fnmatch(file_path, pattern)
                                for pattern in exclude_patterns
                            ):
                                if log_skipped:
                                    logging.info(f"Skipping excluded file: {file_path}")
                                continue
        
                            # Check inclusion patterns for files (if provided)
                            if (
                                include_patterns
                                and not any(
                                    fnmatch.fnmatch(filename, pattern)
                                    for pattern in include_patterns
                                )
                                and not any(
                                    fnmatch.fnmatch(file_path, pattern)
                                    for pattern in include_patterns
                                )
                            ):
                                if log_skipped:
                                    logging.info(
                                        f"Skipping file not in include list: {file_path}"
                                    )
                                continue
        
                            # Ensure we don't include the output file itself
                            if abs_file_path == abs_output_file:
                                if log_skipped:
                                    logging.info(f"Skipping output file itself: {file_path}")
                                continue
        
                            # Write filename
                            outfile.write(f"{sub_indent}{filename}\n")
        
                            # --- Read and Write File Content ---
                            try:
                                # Check file size limit
                                if max_file_size_kb is not None:
                                    file_size = os.path.getsize(file_path)
                                    if file_size > max_file_size_kb * 1024:
                                        outfile.write(
                                            f"{sub_indent}    [CONTENT SKIPPED - File size ({file_size / 1024:.2f} KB) > limit ({max_file_size_kb} KB)]\n\n"
                                        )
                                        if log_skipped:
                                            logging.info(
                                                f"Skipping content (size limit): {file_path}"
                                            )
                                        continue
        
                                # Try reading with UTF-8 first, fallback if needed (already handled by errors='ignore' in open)
                                with open(
                                    file_path, "r", encoding="utf-8", errors="ignore"
                                ) as infile:
                                    content = infile.read()
        
                                # Write content delimiter and content
                                outfile.write(f"{sub_indent}    --- START FILE CONTENT ---\n")
                                # Indent each line of the content
                                for line in content.splitlines():
                                    outfile.write(f"{sub_indent}    {line}\n")
                                outfile.write(f"{sub_indent}    --- END FILE CONTENT ---\n\n")
        
                            except Exception as e:
                                outfile.write(f"{sub_indent}    [ERROR READING FILE: {e}]\n\n")
                                logging.error(
                                    f"Error reading file {file_path}: {e}", exc_info=False
                                )  # Log error without full traceback usually
        
                    outfile.write("=" * 80 + "\n")
                    outfile.write("# End of Project Structure and Content\n")
                logging.info(f"Successfully wrote project structure to {output_file}")
        
            except IOError as e:
                logging.error(f"Error writing to output file {output_file}: {e}")
            except Exception as e:
                logging.error(f"An unexpected error occurred: {e}", exc_info=True)
        
        
        # --- Example Usage ---
        if __name__ == "__main__":
            # Get the directory where this script is located
            script_dir = "/Users/ted/Documents/Studying/LLM/Projects/LangGraphLearning/rag_chatbot_project"
            # Assume the project root is one level up from the script's directory
            # ADJUST THIS PATH if your project structure is different
            project_root_dir = "/Users/ted/Documents/Studying/LLM/Projects/LangGraphLearning/rag_chatbot_project"
        
            # Or specify an absolute path directly:
            # project_root_dir = "/path/to/your/project"
        
            output_filename = "project_snapshot.txt"
            output_filepath = os.path.join(
                script_dir, output_filename
            )  # Save in the same dir as the script
        
            print(f"Project Directory: {project_root_dir}")
            print(f"Output File:       {output_filepath}")
        
            # Define specific patterns to exclude (add more as needed)
            custom_exclude_patterns = [
                ".git*",
                "__pycache__",
                "*.pyc",
                "*.pyo",
                "*.log",
                ".env",
                "venv",
                ".venv",
                "node_modules",
                "dist",
                "build",
                "*.egg-info",
                ".DS_Store",
                ".idea",
                ".vscode",
                "app.log",
                "setup.sh",
                "requirements.txt",
                output_filename,  # Exclude the output file itself
            ]
        
            # Define specific patterns to include (optional)
            # If you only want Python and text files:
            # custom_include_patterns = ["*.py", "*.txt", "*.md", "*.toml"]
            custom_include_patterns = None  # Include everything not excluded
        
            # Set a max file size in KB (e.g., 100KB) or None to include all sizes
            max_size_kb = 100
        
            if os.path.exists(project_root_dir):
                write_project_to_txt(
                    project_dir=project_root_dir,
                    output_file=output_filepath,
                    exclude_patterns=custom_exclude_patterns,
                    include_patterns=custom_include_patterns,
                    max_file_size_kb=max_size_kb,
                )
                print("Done.")
            else:
                print(f"Error: Project directory '{project_root_dir}' does not exist.")
        --- END FILE CONTENT ---

    main.py
        --- START FILE CONTENT ---
        import logging
        import logging.handlers
        import os
        import subprocess
        import sys
        
        try:
            from chatbot.config import setup_logging
        
            logger = setup_logging()
        
        except ImportError:
        
            logging.basicConfig(
                level=logging.INFO,
                format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
            )
            logger = logging.getLogger(__name__)
            logger.warning(
                "main.py: Could not load logging configuration from app.config. Using basic config."
            )
        except Exception as e:
        
            logging.basicConfig(
                level=logging.INFO,
                format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
            )
            logger = logging.getLogger(__name__)
            logger.error(
                f"main.py: Error configuring logging from app.config: {e}", exc_info=True
            )
        
        
        def run_streamlit():
            """Launches the Streamlit application app/app.py using subprocess."""
        
            app_file = os.path.join("chatbot", "app.py")
            logger.info(f"main.py: Looking for application file: {app_file}")
        
            if not os.path.exists(app_file):
                logger.error(
                    f"main.py: Error - Could not find file '{app_file}'. Make sure it exists relative to main.py."
                )
                print(
                    f"Error: Cannot find {app_file}. Please ensure it's in the 'app' subdirectory.",
                    file=sys.stderr,
                )
                sys.exit(1)
        
            command = [sys.executable, "-m", "streamlit", "run", app_file]
            logger.info(f"main.py: Preparing to execute command: {' '.join(command)}")
            print(f"--- Starting Streamlit app ({app_file}) ---")
        
            try:
        
                process = subprocess.run(command, check=True)
        
                logger.info(
                    f"main.py: Streamlit process finished with return code: {process.returncode}"
                )
        
            except subprocess.CalledProcessError as e:
        
                logger.error(
                    f"main.py: Error running Streamlit application: {e}", exc_info=True
                )
                print(f"\n--- Error running Streamlit app ---", file=sys.stderr)
                print(f"Command: {' '.join(command)}", file=sys.stderr)
                print(f"Return code: {e.returncode}", file=sys.stderr)
                sys.exit(e.returncode)
        
            except FileNotFoundError:
        
                logger.error(
                    "main.py: Error - 'streamlit' command or Python executable not found.",
                    exc_info=True,
                )
                print(f"\n--- Error: Could not execute Streamlit command ---", file=sys.stderr)
                print(
                    f"Ensure Streamlit is installed in the Python environment ('{sys.executable}')",
                    file=sys.stderr,
                )
                print(f"and that the Python executable path is correct.", file=sys.stderr)
                print(
                    f"You can install Streamlit using: pip install streamlit", file=sys.stderr
                )
                sys.exit(1)
        
            except KeyboardInterrupt:
        
                logger.info(
                    "main.py: Streamlit application stopped by user (KeyboardInterrupt)."
                )
                print("\n--- Streamlit app interrupted by user ---")
                sys.exit(0)
        
        
        if __name__ == "__main__":
            run_streamlit()
        --- END FILE CONTENT ---

    pyproject.toml
        --- START FILE CONTENT ---
        [build-system]
        requires = ["setuptools>=61.0"]
        build-backend = "setuptools.build_meta"
        
        [tool.black]
        line-length = 88  
        
        [tool.isort]
        profile = "black" 
        line_length = 88  
        multi_line_output = 3
        include_trailing_comma = true
        force_grid_wrap = 0
        use_parentheses = true
        ensure_newline_before_comments = true
        --- END FILE CONTENT ---

[llm/]
    __init__.py
        --- START FILE CONTENT ---
        --- END FILE CONTENT ---

    components.py
        --- START FILE CONTENT ---
        import logging
        import os
        import re
        import tempfile
        import json
        import uuid
        from typing import Any, Dict, List, Optional, Union
        import requests
        
        from chatbot.config import settings
        
        from pinecone import Pinecone
        
        
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_community.document_loaders import (
            ArxivLoader,
            PyPDFLoader,
            UnstructuredFileLoader,
            WebBaseLoader,
        )
        from langchain_community.tools.tavily_search import TavilySearchResults
        from langchain_core.documents import Document
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        from langchain_pinecone import PineconeVectorStore
        
        try:
            from pinecone.openapi_support.exceptions import PineconeApiException
        except ImportError:
            PineconeApiException = Exception
        
        from streamlit.runtime.uploaded_file_manager import UploadedFile
        
        
        logger = logging.getLogger("RAG_Chatbot_App")
        
        
        def _load_arxiv_source(paper_id: str) -> List[Document]:
            """Loads a document from arXiv using its ID."""
            logger.debug(f"Using ArxivLoader for ID: {paper_id}")
            try:
                loader = ArxivLoader(
                    query=paper_id, load_max_docs=1, load_all_available_meta=True
                )
                docs = loader.load()
                logger.debug(
                    f"Finished loading from arXiv: {paper_id}. Documents loaded: {len(docs)}"
                )
                return docs
            except Exception as e:
                logger.error(f"ArxivLoader failed for ID '{paper_id}': {e}", exc_info=True)
                return []
        
        
        def _load_pdf_from_url(url: str, temp_files_list: List[str]) -> List[Document]:
            """Downloads a PDF from a URL and loads it using PyPDFLoader."""
            logger.debug(f"Downloading direct PDF link: {url}")
            file_path = None
            docs = []
            try:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    response = requests.get(url, stream=True, timeout=30)
                    response.raise_for_status()
                    for chunk in response.iter_content(chunk_size=8192):
                        tmp_file.write(chunk)
                    file_path = tmp_file.name
                temp_files_list.append(file_path)
                logger.debug(f"Downloaded PDF to temporary file: {file_path}")
                loader = PyPDFLoader(file_path)
                docs = loader.load()
        
                for doc in docs:
                    if "source" not in doc.metadata:
                        doc.metadata["source"] = url
                logger.debug(
                    f"Finished loading from downloaded PDF: {url}. Pages loaded: {len(docs)}"
                )
            except requests.exceptions.RequestException as req_err:
                logger.error(f"Failed to download PDF URL '{url}': {req_err}", exc_info=True)
            except Exception as pdf_err:
                logger.error(
                    f"Failed to process downloaded PDF '{url}': {pdf_err}", exc_info=True
                )
        
            return docs
        
        
        def _load_web_html(url: str) -> List[Document]:
            """Loads content from a general web URL, likely HTML."""
            logger.debug(f"Assuming HTML, using WebBaseLoader for URL: {url}")
            try:
                loader = WebBaseLoader([url])
                docs = loader.load()
                logger.debug(
                    f"Finished loading from WebBaseLoader URL: {url}. Documents loaded: {len(docs)}"
                )
                return docs
            except Exception as web_err:
                logger.error(f"WebBaseLoader failed for URL '{url}': {web_err}", exc_info=True)
                return []
        
        
        def _load_local_file(file_path: str) -> List[Document]:
            """Loads a document from a local file path (PDF or other)."""
            logger.debug(f"Loading from local file path: {file_path}")
            try:
                _, ext = os.path.splitext(file_path)
                if ext.lower() == ".pdf":
                    loader = PyPDFLoader(file_path)
                else:
                    logger.warning(f"Local file is not PDF, trying Unstructured: {file_path}")
                    loader = UnstructuredFileLoader(file_path)
                docs = loader.load()
                logger.debug(
                    f"Finished loading from local file path: {file_path}. Documents loaded: {len(docs)}"
                )
                return docs
            except Exception as e:
                logger.error(f"Failed loading local file '{file_path}': {e}", exc_info=True)
                return []
        
        
        def _load_uploaded_file(
            uploaded_file: UploadedFile, temp_files_list: List[str]
        ) -> List[Document]:
            """Loads data from a Streamlit UploadedFile object."""
            file_name = uploaded_file.name
            logger.debug(f"Processing UploadedFile: {file_name}")
            file_path = None
            docs = []
            try:
                with tempfile.NamedTemporaryFile(
                    delete=False, suffix=os.path.splitext(file_name)[1]
                ) as tmp_file:
                    tmp_file.write(uploaded_file.getbuffer())
                    file_path = tmp_file.name
                temp_files_list.append(file_path)
                logger.debug(f"Saved UploadedFile to temp: {file_path}")
                _, ext = os.path.splitext(file_path)
                if ext.lower() == ".pdf":
                    loader = PyPDFLoader(file_path)
                else:
                    logger.warning(f"Uploaded file not PDF, trying Unstructured: {file_name}")
                    loader = UnstructuredFileLoader(file_path)
                docs = loader.load()
        
                for doc in docs:
                    doc.metadata["source"] = file_name
                logger.debug(
                    f"Finished loading from UploadedFile temp: {file_name}. Documents loaded: {len(docs)}"
                )
            except Exception as e:
                logger.error(
                    f"Failed processing UploadedFile '{file_name}': {e}", exc_info=True
                )
        
            return docs
        
        
        def _clean_document_text(doc: Document) -> Document:
            """Normalizes whitespace in the document's page_content."""
            if isinstance(doc.page_content, str):
                text = doc.page_content
        
                cleaned_text = re.sub(r"\s+", " ", text).strip()
                if cleaned_text != text:
                    logger.debug(
                        f"Cleaned whitespace in doc from source: {doc.metadata.get('source', 'N/A')[:50]}..."
                    )
                return Document(page_content=cleaned_text, metadata=doc.metadata)
            else:
                logger.warning(
                    f"Doc page_content is not a string, skipping cleaning. Type: {type(doc.page_content)}"
                )
                return doc
        
        
        def _cleanup_temp_files(temp_files: List[str]):
            """Removes temporary files created during loading."""
            logger.debug(f"Cleaning up {len(temp_files)} temporary files...")
            for temp_file in temp_files:
                try:
                    os.remove(temp_file)
                    logger.debug(f"Deleted temporary file: {temp_file}")
                except Exception as e:
                    logger.warning(f"Could not delete temporary file {temp_file}: {e}")
        
        
        def load_and_split(sources: List[Union[str, UploadedFile]]) -> List[Document]:
            """
            Loads, cleans, and splits documents from various sources.
            """
            logger.info(f"Starting load, clean, and split for {len(sources)} sources.")
            all_loaded_docs = []
            temp_files_to_clean = []
            url_pattern = re.compile(r"https?://\S+")
            arxiv_pattern = re.compile(
                r"https?://arxiv\.org/(?:abs|pdf)/(\d+\.\d+(?:v\d+)?)(?:\.pdf)?$"
            )
        
            for source_num, source in enumerate(sources):
                docs_from_source = []
                try:
                    if isinstance(source, str) and url_pattern.match(source):
                        arxiv_match = arxiv_pattern.match(source)
                        if arxiv_match:
                            docs_from_source = _load_arxiv_source(arxiv_match.group(1))
                        elif source.lower().endswith(".pdf"):
                            docs_from_source = _load_pdf_from_url(source, temp_files_to_clean)
                        else:
                            docs_from_source = _load_web_html(source)
                    elif isinstance(source, str):
                        docs_from_source = _load_local_file(source)
                    elif isinstance(source, UploadedFile):
                        docs_from_source = _load_uploaded_file(source, temp_files_to_clean)
                    else:
                        logger.warning(
                            f"Source {source_num+1}: Unknown type '{type(source)}', skipping."
                        )
        
                    cleaned_docs = [_clean_document_text(doc) for doc in docs_from_source]
                    all_loaded_docs.extend(cleaned_docs)
        
                except Exception as e:
        
                    logger.error(
                        f"Error processing source {source_num+1} ('{str(source)[:50]}...'): {e}",
                        exc_info=True,
                    )
        
            if not all_loaded_docs:
                logger.warning(
                    "No documents were loaded successfully after processing all sources."
                )
                _cleanup_temp_files(temp_files_to_clean)
                return []
        
            logger.info(f"Total documents loaded and cleaned: {len(all_loaded_docs)}")
        
            logger.debug(
                f"Splitting {len(all_loaded_docs)} documents (chunk_size={settings.CHUNK_SIZE}, overlap={settings.CHUNK_OVERLAP})..."
            )
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.CHUNK_SIZE,
                chunk_overlap=settings.CHUNK_OVERLAP,
                separators=["\n\n", "\n", ". ", " ", ""],
            )
            try:
                split_docs = text_splitter.split_documents(all_loaded_docs)
                logger.info(f"Split into {len(split_docs)} chunks.")
            except Exception as split_err:
                logger.error(f"Error splitting documents: {split_err}", exc_info=True)
                _cleanup_temp_files(temp_files_to_clean)
                return []
        
            if not split_docs:
                logger.warning("Splitting resulted in zero documents.")
                _cleanup_temp_files(temp_files_to_clean)
                return []
        
            if split_docs:
                logger.debug(
                    f"First chunk content preview (cleaned): {split_docs[0].page_content[:300]}..."
                )
                logger.debug(f"Metadata of first chunk: {split_docs[0].metadata}")
        
            _cleanup_temp_files(temp_files_to_clean)
            return split_docs
        
        
        def clean_metadata_for_pinecone(metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:
            """
            Cleans a metadata dictionary to be compatible with Pinecone.
            Removes Nones, truncates long strings, attempts conversion, checks total size.
            """
            if not metadata:
                return {}
        
            if "text" in metadata:
                logger.warning(
                    "Removing unexpected 'text' key found in metadata during cleaning."
                )
                metadata = metadata.copy()
                del metadata["text"]
        
            cleaned_metadata = {}
            estimated_size = 0
            allowed_types = (str, bool, int, float)
            keys_to_prune_on_size_limit = ["summary", "authors"]
        
            keys_to_process = list(metadata.keys())
        
            for key in keys_to_process:
                value = metadata.get(key)
                if value is None:
                    continue
        
                cleaned_value: Any = None
        
                if isinstance(value, allowed_types):
                    if isinstance(value, str):
                        cleaned_value = value[: settings.MAX_METADATA_FIELD_LENGTH]
                        if len(value) > settings.MAX_METADATA_FIELD_LENGTH:
                            logger.warning(f"Truncated metadata string field '{key}'.")
                    else:
                        cleaned_value = value
        
                elif isinstance(value, list):
                    str_list = []
                    valid_list = True
                    for item in value:
                        if isinstance(item, str):
                            str_list.append(item[: settings.MAX_METADATA_FIELD_LENGTH])
                        else:
                            valid_list = False
                            break
                    if valid_list and str_list:
                        cleaned_value = str_list
                    else:
                        logger.warning(
                            f"Skipping metadata list '{key}' (non-string item or empty)."
                        )
                        continue
        
                else:
                    logger.warning(
                        f"Metadata field '{key}' type {type(value)}. Converting to string."
                    )
                    try:
                        cleaned_value = str(value)[: settings.MAX_METADATA_FIELD_LENGTH]
                    except Exception:
                        logger.warning(
                            f"Could not convert metadata '{key}' to string. Skipping."
                        )
                        continue
        
                if cleaned_value is not None:
                    cleaned_metadata[key] = cleaned_value
        
            try:
                metadata_json_string = json.dumps(cleaned_metadata)
                estimated_size = len(metadata_json_string.encode("utf-8"))
            except Exception as json_err:
                logger.warning(
                    f"Could not estimate metadata size using JSON for pruning: {json_err}"
                )
                estimated_size = settings.MAX_TOTAL_METADATA_BYTES + 1
        
            if estimated_size > settings.MAX_TOTAL_METADATA_BYTES:
                logger.warning(
                    f"Estimated metadata size ({estimated_size} bytes) exceeds limit ({settings.MAX_TOTAL_METADATA_BYTES} bytes). Pruning..."
                )
        
                for prune_key in keys_to_prune_on_size_limit:
                    if prune_key in cleaned_metadata:
                        del cleaned_metadata[prune_key]
                        logger.info(f"Pruned metadata field '{prune_key}'.")
                        try:
                            estimated_size = len(json.dumps(cleaned_metadata).encode("utf-8"))
                        except Exception:
                            pass
                        if estimated_size <= settings.MAX_TOTAL_METADATA_BYTES:
                            break
                if estimated_size > settings.MAX_TOTAL_METADATA_BYTES:
                    logger.error(
                        f"Metadata size ({estimated_size}) STILL exceeds limit after pruning."
                    )
        
            return cleaned_metadata
        
        
        def get_pinecone_vectorstore(index_name: str) -> Optional[PineconeVectorStore]:
            """Initializes connection to an existing Pinecone index."""
            logger.info(f"Initializing connection to Pinecone index: {index_name}")
            if not index_name:
                logger.error("Pinecone index name not provided.")
                return None
            try:
                pc = Pinecone(api_key=settings.PINECONE_API_KEY)
                index = pc.Index(settings.PINECONE_INDEX_NAME)
                embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
                vectorstore = PineconeVectorStore(index=index, embedding=embedding_model)
        
                if not hasattr(vectorstore, "index_name"):
                    vectorstore.index_name = index_name
                logger.info(f"Connected to Pinecone index '{index_name}' successfully.")
                return vectorstore
            except Exception as e:
                logger.error(
                    f"Error initializing Pinecone VectorStore for index '{index_name}': {e}",
                    exc_info=True,
                )
                return None
        
        
        def embed_and_upsert(docs: List[Document], vectorstore: PineconeVectorStore) -> bool:
            batch = []
            total_size = 0
        
            def flush_batch():
                if batch:
                    vectorstore.add_documents(batch)
                    batch.clear()
        
            index_name = getattr(vectorstore, "index_name", "[Unknown Index]")
            print(f"index name: {index_name}")
            docs_to_upsert = []
        
            for doc in docs:
                cleaned_meta = clean_metadata_for_pinecone(doc.metadata or {})
                batch.append(Document(page_content=doc.page_content, metadata=cleaned_meta))
                # print(doc)
                total_size += len(doc.page_content.encode("utf-8"))
        
                if total_size >= 3 * 1024 * 1024 or len(batch) >= settings.PINECONE_BATCH_SIZE:
                    flush_batch()
                    total_size = 0
        
            flush_batch()
            return True
        
        
        # def embed_and_upsert(docs: List[Document], vectorstore: PineconeVectorStore) -> bool:
        #     """
        #     Embeds documents and upserts them ONE BY ONE to Pinecone after cleaning metadata.
        #     Uses direct index.upsert for debugging. Catches correct Pinecone exception.
        #     """
        #     if not docs: logger.warning("No documents to embed and upsert."); return False
        #     if not vectorstore: logger.error("Invalid vectorstore provided."); return False
        #     # Safely access the index object
        #     try:
        #         index = vectorstore.index
        #         if index is None: raise AttributeError("vectorstore.index is None")
        #     except AttributeError:
        #          logger.error("Cannot access the raw Pinecone index object via vectorstore.index.")
        #          return False
        #     # Safely access the embedding function
        #     try:
        #         # Correct attribute is likely embeddings, not embedding
        #         embedder = vectorstore.embeddings
        #         if embedder is None: raise AttributeError("vectorstore.embeddings is None")
        #     except AttributeError:
        #         logger.error("Cannot access the embedding function via vectorstore.embeddings. Initializing fallback.")
        #         try:
        #             # Ensure API key is available for fallback
        #             if not settings.OPENAI_API_KEY: raise ValueError("OPENAI_API_KEY needed for fallback embedder")
        #             embedder = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=settings.OPENAI_API_KEY)
        #         except Exception as init_err:
        #              logger.error(f"Failed to initialize fallback OpenAIEmbeddings: {init_err}", exc_info=True)
        #              return False
        
        
        #     index_name = getattr(vectorstore, 'index_name', '[Unknown Index]')
        #     logger.info(f"Starting MANUAL embed/upsert of {len(docs)} chunks one-by-one to index '{index_name}'...")
        
        #     success_count = 0
        #     fail_count = 0
        #     # Use the namespace associated with the vectorstore object if available
        #     # Note: _namespace might be internal, check Langchain docs for public access method if needed
        #     namespace = getattr(vectorstore, '_namespace', None)
        #     logger.info(f"Using Pinecone namespace: '{namespace}'")
        
        #     for i, doc in enumerate(docs):
        #         logger.debug(f"Processing document {i+1}/{len(docs)}...")
        
        #         # 1. Clean Metadata
        #         cleaned_metadata = clean_metadata_for_pinecone(doc.metadata or {})
        #         logger.debug(f"Cleaned metadata for doc {i}: {cleaned_metadata}")
        
        #         # 2. Generate Embedding
        #         try:
        #             # Use embed_query for single text embedding
        #             vector = embedder.embed_query(doc.page_content)
        #             logger.debug(f"Generated embedding for doc {i} (size: {len(vector)})")
        #         except Exception as embed_err:
        #             logger.error(f"Failed to generate embedding for doc {i}: {embed_err}", exc_info=True)
        #             fail_count += 1
        #             continue # Skip to the next document
        
        #         # 3. Generate ID
        #         doc_id = uuid.uuid4().hex
        #         logger.debug(f"Generated ID for doc {i}: {doc_id}")
        
        #         # 4. Prepare vector tuple for Pinecone upsert
        #         vector_tuple = (doc_id, vector, cleaned_metadata)
        
        #         # 5. Upsert SINGLE vector
        #         try:
        #             logger.debug(f"Attempting to upsert vector for doc {i} (ID: {doc_id}) into namespace '{namespace}'...")
        #             # Use the raw index object's upsert method
        #             upsert_response = index.upsert(vectors=[vector_tuple], namespace=namespace)
        #             logger.debug(f"Upsert response for doc {i}: {upsert_response}")
        #             success_count += 1
        #         except PineconeApiException as pinecone_err:
        #             # Catch the CORRECTED Pinecone specific API errors
        #             logger.error(f"Pinecone API Error upserting doc {i} (ID: {doc_id}): {pinecone_err}", exc_info=True)
        #             logger.error(f"--- Failing vector metadata (doc {i}): {cleaned_metadata}")
        #             logger.error(f"--- Failing vector content preview (doc {i}): {doc.page_content[:300]}...")
        #             fail_count += 1
        #             # break # Optional: Stop on first error
        #         except Exception as other_err:
        #             # Catch other potential errors during upsert
        #             logger.error(f"Unexpected Error upserting doc {i} (ID: {doc_id}): {other_err}", exc_info=True)
        #             fail_count += 1
        #             # break # Optional: Stop on first error
        
        #     logger.info(f"Manual upsert process finished. Successful: {success_count}, Failed: {fail_count}")
        #     # Return True only if all documents were successfully upserted
        #     return fail_count == 0
        
        
        def similarity_search(
            query: str, vectorstore: PineconeVectorStore, k: int = settings.TOP_K
        ) -> str:
            """Performs similarity search and returns formatted context string."""
            if not vectorstore:
                logger.error("Invalid vectorstore for similarity_search.")
                return ""
            index_name = getattr(vectorstore, "index_name", "[Unknown Index]")
            logger.info(
                f"Performing similarity search (k={k}) on index '{index_name}' for query: '{query[:50]}...'"
            )
            try:
                retrieved_docs = vectorstore.similarity_search(query, k=k)
                if retrieved_docs:
                    context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
                    logger.info(
                        f"Similarity search found {len(retrieved_docs)} relevant chunks."
                    )
                    return context
                else:
                    logger.info("Similarity search found no matching chunks.")
                    return ""
            except Exception as e:
                logger.error(f"Error during similarity search: {e}", exc_info=True)
                return ""
        
        
        def web_search(query: str, max_results: int = settings.TAVILY_MAX_RESULTS) -> str:
            """Performs web search using Tavily and returns formatted results string."""
            logger.info(
                f"Performing web search via Tavily (max_results={max_results}) for query: '{query[:50]}...'"
            )
            try:
                tavily_tool = TavilySearchResults(max_results=max_results)
                results = tavily_tool.invoke(query)
                if results and isinstance(results, list):
                    formatted_results = "\n\n---\n\n".join(
                        [
                            f"Source: {res.get('url', 'N/A')}\nContent: {res.get('content', '')}"
                            for res in results
                            if isinstance(res, dict)
                        ]
                    )
                    logger.info(f"Web search found {len(results)} results.")
                    return formatted_results
                else:
                    logger.info("Web search found no results or invalid format.")
                    return ""
            except Exception as e:
                logger.error(f"Error during Tavily search: {e}", exc_info=True)
                return ""
        
        
        def get_llm() -> Optional[ChatOpenAI]:
            """Initializes and returns the ChatOpenAI LLM instance."""
            logger.debug("Initializing ChatOpenAI LLM...")
            try:
                llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)
                logger.info("ChatOpenAI LLM initialized.")
                return llm
            except Exception as e:
                logger.error(f"Error initializing ChatOpenAI: {e}", exc_info=True)
                return None
        
        
        def create_rag_prompt() -> ChatPromptTemplate:
            """Creates the prompt template for RAG (English)."""
            logger.debug("Creating prompt template for RAG.")
            template = """You are a helpful AI assistant. Your task is to answer the user's question based PRIMARILY on the provided context below.
        If the information is not available in the context, state that you could not find the information in the provided documents.
        Answer politely and in detail. DO NOT make up information. Always answer in English.
        
        Context:
        {context}
        
        Chat History:
        {chat_history}
        
        User Question: {query}
        
        Your Answer (English):"""
            return ChatPromptTemplate.from_template(template)
        
        
        def create_web_search_prompt() -> ChatPromptTemplate:
            """Creates the prompt template for web search results (English)."""
            logger.debug("Creating prompt template for Web Search.")
            template = """You are a helpful AI assistant. Your task is to answer the user's question based PRIMARILY on the web search results provided below.
        Synthesize the information from the search results to provide a comprehensive answer.
        If the information is not available in the search results, state that you could not find relevant information on the web.
        Answer politely and in detail. DO NOT make up information. Always answer in English.
        
        Web Search Results:
        {context}
        
        Chat History:
        {chat_history}
        
        User Question: {query}
        
        Your Answer (English):"""
            return ChatPromptTemplate.from_template(template)
        
        
        def create_generation_chain(llm: ChatOpenAI, prompt: ChatPromptTemplate) -> Any:
            """Creates a LangChain Runnable (LCEL chain) for generating responses."""
            logger.debug("Creating generation chain (prompt | llm | StrOutputParser).")
            if not llm or not prompt:
                logger.error("Invalid LLM or Prompt provided...")
                raise ValueError("LLM and Prompt are required...")
            return prompt | llm | StrOutputParser()
        
        
        def format_chat_history(chat_history: List[Dict[str, str]]) -> str:
            """Formats chat history into a readable string for the prompt (English)."""
            if not chat_history:
                return "No chat history."
            formatted = []
            for msg in chat_history:
                role = "User" if msg.get("role") == "user" else "Assistant AI"
                content = msg.get("content", "")
                formatted.append(f"{role}: {content}")
            return "\n".join(formatted)
        --- END FILE CONTENT ---

    graph_logic.py
        --- START FILE CONTENT ---
        import logging
        from typing import Any, Dict, Generator, List, Literal, Optional, TypedDict
        
        import streamlit as st
        from langgraph.graph import END, StateGraph
        
        
        try:
            from .components import (
                create_generation_chain,
                create_rag_prompt,
                create_web_search_prompt,
                format_chat_history,
                get_llm,
                similarity_search,
                web_search,
            )
        except ImportError:
        
            try:
                from components import (
                    create_generation_chain,
                    create_rag_prompt,
                    create_web_search_prompt,
                    format_chat_history,
                    get_llm,
                    similarity_search,
                    web_search,
                )
            except ImportError as e:
                logging.error(
                    f"Failed to import components for graph_logic: {e}", exc_info=True
                )
        
                try:
                    st.error(
                        f"Critical Import Error: Could not load components for graph logic. Details: {e}"
                    )
                    st.stop()
                except Exception:
                    raise ImportError(
                        f"Could not import components needed by graph_logic: {e}"
                    ) from e
        
        
        logger = logging.getLogger("RAG_Chatbot_App")
        
        
        class GraphState(TypedDict):
            """Defines the state passed between nodes in the graph."""
        
            query: str
            chat_history: List[dict]
            chatbot_mode: str
            use_rag: bool
            context: Optional[str]
            generation: Optional[str]
            generation_stream: Optional[Generator[Any, None, None]]
            status_message: str
        
        
        def route_query(
            state: GraphState,
        ) -> Literal["vectorstore", "web_search"]:
            """Routes the query based ONLY on the selected chatbot mode."""
            mode = state.get("chatbot_mode")
            vectorstore_ready = state.get("use_rag", False)
            logger.info(
                f"Routing query - Mode Selected: {mode}, Vectorstore Initialized: {vectorstore_ready}"
            )
        
            if mode == "RAG":
        
                if not vectorstore_ready:
        
                    logger.warning(
                        "RAG mode selected, but Pinecone vectorstore is not initialized in session state. Proceeding to retrieve_context anyway."
                    )
                logger.info("RAG Mode selected -> Route: vectorstore")
                return "vectorstore"
            elif mode == "Web Search":
                logger.info("Web Search Mode selected -> Route: web_search")
                return "web_search"
            else:
        
                logger.warning(
                    f"Unknown chatbot mode: '{mode}'. Defaulting route to 'web_search'."
                )
                return "web_search"
        
        
        def inform_no_rag_data(state: GraphState) -> Dict[str, Any]:
            """Generates a notification stream when RAG mode lacks data."""
            logger.info("Executing 'inform_no_rag_data' node.")
            message = "You selected RAG mode, but no data sources have been processed yet. Please upload and process documents first, or switch to 'Web Search (Tavily)' mode."
            status = "Missing data for RAG mode"
        
            def single_message_stream(msg: str):
                yield msg
        
            return {
                "generation_stream": single_message_stream(message),
                "status_message": status,
                "context": "",
            }
        
        
        def retrieve_context(state: GraphState) -> Dict[str, Any]:
            """Retrieves context from the vector store for RAG."""
            logger.info("Executing 'retrieve_context' node (RAG).")
            query = state["query"]
            vectorstore = st.session_state.get("vectorstore", None)
            status_update = "Querying RAG Vector Store..."
            context = ""
        
            if vectorstore:
                try:
                    context = similarity_search(query, vectorstore)
                    if context:
                        status_update = "Retrieved context from RAG Vector Store."
                        logger.info(
                            f"RAG context retrieval successful (length {len(context)})."
                        )
                    else:
                        status_update = "No relevant RAG context found."
                        logger.info("No relevant RAG context found.")
                except Exception as e:
                    logger.error(f"Error during RAG query: {e}", exc_info=True)
                    status_update = f"Error querying RAG: {e}"
                    context = ""
            else:
                logger.error("RAG Vector Store not available in retrieve_context node.")
                status_update = "Error: RAG Vector Store not ready."
        
            return {"context": context or "", "status_message": status_update}
        
        
        def search_web(state: GraphState) -> Dict[str, Any]:
            """Performs web search using Tavily."""
            logger.info("Executing 'search_web' node.")
            query = state["query"]
            status_update = "Searching the web..."
            context = ""
            try:
                context = web_search(query)
                if context:
                    status_update = "Retrieved information from web search."
                    logger.info(f"Web search successful (length {len(context)}).")
                else:
                    status_update = "No relevant information found on the web."
                    logger.info("Web search returned no results.")
            except Exception as e:
                logger.error(f"Error during web search: {e}", exc_info=True)
                status_update = f"Error searching web: {e}"
                context = ""
        
            return {"context": context or "", "status_message": status_update}
        
        
        def generate_answer(state: GraphState) -> Dict[str, Any]:
            """Generates the final answer stream based on context and mode."""
            logger.info("Executing 'generate_answer' node.")
            query = state["query"]
            context = state.get("context", "")
            chat_history_list = state.get("chat_history", [])
            mode = state.get("chatbot_mode")
        
            llm = get_llm()
            if not llm:
                logger.error("LLM not available in generate_answer node.")
        
                def error_stream():
                    yield "Error: Language Model is unavailable."
        
                return {
                    "generation_stream": error_stream(),
                    "status_message": "LLM Initialization Error",
                }
        
            formatted_history = format_chat_history(chat_history_list)
        
            if mode == "RAG":
                logger.debug("Using RAG prompt for generation.")
                prompt_template = create_rag_prompt()
            elif mode == "Web Search":
                logger.debug("Using Web Search prompt for generation.")
                prompt_template = create_web_search_prompt()
            else:
                logger.warning(
                    f"Unknown mode '{mode}' in generate node, using Web Search prompt."
                )
                prompt_template = create_web_search_prompt()
        
            try:
                generation_chain = create_generation_chain(llm, prompt_template)
            except ValueError as e:
                logger.error(f"Failed to create generation chain: {e}", exc_info=True)
        
                def error_stream():
                    yield f"Error: Could not create generation chain - {e}"
        
                return {
                    "generation_stream": error_stream(),
                    "status_message": "Chain Creation Error",
                }
        
            chain_input = {
                "query": query,
                "context": context if context else "No context provided.",
                "chat_history": formatted_history,
            }
            logger.debug(f"Input context length for LLM: {len(chain_input['context'])}")
        
            stream_to_return = iter([])
            final_status = "Starting answer generation..."
            try:
                logger.info("Calling stream on generation_chain...")
                _raw_stream = generation_chain.stream(chain_input)
                if hasattr(_raw_stream, "__iter__") or hasattr(_raw_stream, "__aiter__"):
                    stream_to_return = _raw_stream
                    final_status = "Receiving data from LLM..."
                    logger.info("Successfully started streaming from LLM.")
                else:
                    logger.error(
                        f"generation_chain.stream did not return an iterator! Type: {type(_raw_stream)}"
                    )
                    final_status = "Error: Did not receive a valid stream from LLM."
            except Exception as e:
                logger.error(f"Error calling generation_chain.stream: {e}", exc_info=True)
                final_status = f"Error generating answer: {e}"
        
            return {
                "generation_stream": stream_to_return,
                "status_message": final_status,
            }
        
        
        def build_graph() -> StateGraph:
            logger.info("Building the application graph...")
            graph = StateGraph(GraphState)
        
            graph.add_node("retrieve", retrieve_context)
            graph.add_node("web_search", search_web)
            graph.add_node("generate", generate_answer)
            graph.add_node("inform_no_rag_data", inform_no_rag_data)
        
            logger.debug(
                "Adding conditional edges from entry point (__start__) based on route_query."
            )
            graph.add_conditional_edges(
                "__start__",
                route_query,
                {
                    "vectorstore": "retrieve",
                    "web_search": "web_search",
                },
            )
        
            graph.add_edge("retrieve", "generate")
            graph.add_edge("web_search", "generate")
        
            graph.add_edge("generate", END)
        
            try:
                app_graph = graph.compile()
                logger.info("Graph compiled successfully with updated RAG routing.")
                return app_graph
            except Exception as e:
                logger.error(f"Failed to compile the graph: {e}", exc_info=True)
                try:
                    st.error(
                        f"Critical Error: Failed to compile application graph. Details: {e}"
                    )
                    st.stop()
                except Exception:
                    raise RuntimeError(f"Failed to compile application graph: {e}") from e
        --- END FILE CONTENT ---

[chatbot/]
    __init__.py
        --- START FILE CONTENT ---
        --- END FILE CONTENT ---

    app.py
        --- START FILE CONTENT ---
        import logging
        from typing import Any, Dict, Generator
        import streamlit as st
        
        from chatbot.config import setup_logging, settings
        from chatbot.processing import process_rag_sources
        from chatbot.state_manager import initialize_session_state
        from chatbot.ui_chat import display_chat_history, display_ai_message_elements
        from chatbot.ui_sidebar import display_sidebar
        
        from llm.graph_logic import GraphState
        
        logger = setup_logging()
        
        
        if "PINECONE_INDEX_NAME" not in st.session_state:
            st.session_state.PINECONE_INDEX_NAME = settings.PINECONE_INDEX_NAME
        
        initialize_session_state()
        
        st.set_page_config(page_title="Versatile Chatbot", layout="wide")
        st.title("Versatile Chatbot (RAG / Web Search)")
        
        
        uploaded_files, urls, process_button = display_sidebar()
        
        
        if process_button:
            process_rag_sources(uploaded_files, urls)
        
        display_mode = (
            "Web Search (Tavily)"
            if st.session_state.mode_internal == "Web Search"
            else "RAG (Uploaded Documents)"
        )
        st.markdown(f"**Current Mode:** {display_mode}")
        
        display_chat_history()
        
        if prompt := st.chat_input("Enter your question here..."):
            logger.info(
                f"Received question (Mode: {st.session_state.mode_internal}, Length: {len(prompt)})."
            )
        
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user", avatar="👤"):
                st.markdown(prompt)
        
            with st.chat_message("ai", avatar="🤖"):
        
                response_placeholder, _, status_list_placeholder = display_ai_message_elements()
                current_run_statuses = []
        
                vectorstore_is_ready = st.session_state.vectorstore is not None
                chat_history_for_graph = st.session_state.messages[:-1]
                graph_input: GraphState = {
                    "query": prompt,
                    "chat_history": chat_history_for_graph,
                    "chatbot_mode": st.session_state.mode_internal,
                    "use_rag": vectorstore_is_ready,
                    "context": None,
                    "generation": None,
                    "generation_stream": None,
                    "status_message": "Processing started...",
                }
                logger.debug(
                    f"Graph input: mode='{graph_input['chatbot_mode']}', query='{graph_input['query'][:50]}...', vs_ready={vectorstore_is_ready}"
                )
        
                compiled_graph = st.session_state.graph
                full_response = ""
                graph_error = None
        
                try:
                    initial_status = "Processing started..."
                    current_run_statuses.append(initial_status)
                    status_list_placeholder.markdown(f"- {initial_status}")
        
                    logger.info(
                        f"Starting graph stream execution (Mode: {st.session_state.mode_internal})..."
                    )
        
                    events: Generator[Dict[str, Any], None, None] = compiled_graph.stream(
                        graph_input, config={"recursion_limit": 10}
                    )
        
                    for event in events:
                        logger.debug(f"Graph event: {event}")
                        event_keys = list(event.keys())
        
                        if not event_keys or event_keys[0] == "__end__":
                            continue
        
                        node_name = event_keys[0]
                        logger.debug(f"Processing node: '{node_name}'")
                        node_output = event.get(node_name)
        
                        new_status_message = None
                        if isinstance(node_output, dict):
                            status_message = node_output.get("status_message")
        
                            if status_message and (
                                not current_run_statuses
                                or status_message != current_run_statuses[-1]
                            ):
                                logger.info(f"[Graph Status] {status_message}")
        
                                new_status_message = status_message
                        if new_status_message:
                            current_run_statuses.append(new_status_message)
                            status_markdown = "\n".join(
                                [f"- {s}" for s in current_run_statuses]
                            )
                            status_list_placeholder.markdown(status_markdown)
        
                        if node_name in ["generate", "inform_no_rag_data"]:
                            logger.debug(f"Processing output from node '{node_name}'.")
                            generation_stream = (
                                node_output.get("generation_stream")
                                if isinstance(node_output, dict)
                                else None
                            )
        
                            if generation_stream and (
                                hasattr(generation_stream, "__iter__")
                                or hasattr(generation_stream, "__aiter__")
                            ):
                                try:
                                    logger.debug(f"Iterating stream from '{node_name}'...")
                                    chunk_count = 0
                                    for chunk in generation_stream:
                                        chunk_count += 1
        
                                        chunk_content = (
                                            chunk.content
                                            if hasattr(chunk, "content")
                                            and isinstance(chunk.content, str)
                                            else chunk if isinstance(chunk, str) else None
                                        )
                                        if chunk_content:
                                            full_response += chunk_content
                                            response_placeholder.markdown(full_response + "▌")
                                        elif chunk:
                                            logger.warning(
                                                f"Unexpected chunk type/content from '{node_name}': {type(chunk)}"
                                            )
                                    logger.debug(
                                        f"Finished stream from '{node_name}'. Chunks: {chunk_count}"
                                    )
                                    if chunk_count == 0:
                                        logger.warning(
                                            f"Stream from '{node_name}' yielded no chunks."
                                        )
                                except Exception as stream_ex:
                                    logger.error(
                                        f"Error iterating stream from '{node_name}': {stream_ex}",
                                        exc_info=True,
                                    )
                                    graph_error = stream_ex
        
                                    error_status_stream = (
                                        f"❌ Error reading AI stream: {stream_ex}"
                                    )
                                    if (
                                        not current_run_statuses
                                        or error_status_stream != current_run_statuses[-1]
                                    ):
                                        current_run_statuses.append(error_status_stream)
                                        status_list_placeholder.markdown(
                                            "\n".join([f"- {s}" for s in current_run_statuses])
                                        )
                            elif generation_stream:
                                logger.error(
                                    f"'generation_stream' from '{node_name}' not iterator: {type(generation_stream)}"
                                )
                            else:
                                logger.warning(
                                    f"No valid 'generation_stream' in '{node_name}' output."
                                )
        
                    logger.info("Finished processing graph stream events.")
        
                    if graph_error:
                        final_status_msg = f"❌ An error occurred: {graph_error}"
        
                    else:
                        final_status_msg = "✅ Completed!"
        
                    if not full_response:
                        if any("Missing data for RAG mode" in s for s in current_run_statuses):
                            logger.info("Completed with missing RAG data notification.")
        
                        elif not graph_error:
                            logger.warning(
                                "No response content accumulated despite no apparent error."
                            )
                            full_response = "Sorry, I could not generate an answer."
                        else:
                            full_response = f"Sorry, an error occurred: {graph_error}"
        
                    if not current_run_statuses or final_status_msg != current_run_statuses[-1]:
                        current_run_statuses.append(final_status_msg)
                        status_list_placeholder.markdown(
                            "\n".join([f"- {s}" for s in current_run_statuses])
                        )
        
                    response_placeholder.markdown(full_response)
        
                except Exception as e:
        
                    graph_error = e
                    error_message = f"Critical error running graph: {graph_error}"
                    logger.error(error_message, exc_info=True)
                    final_status_msg = f"❌ {error_message}"
        
                    if not current_run_statuses or final_status_msg != current_run_statuses[-1]:
                        current_run_statuses.append(final_status_msg)
                        status_list_placeholder.markdown(
                            "\n".join([f"- {s}" for s in current_run_statuses])
                        )
        
                    full_response = f"Sorry, a critical error occurred: {graph_error}"
                    response_placeholder.markdown(full_response)
        
                logger.info(
                    f"Final response generated (Mode: {st.session_state.mode_internal}, Length: {len(full_response)})."
                )
                st.session_state.messages.append(
                    {"role": "assistant", "content": full_response}
                )
        --- END FILE CONTENT ---

    config.py
        --- START FILE CONTENT ---
        import logging
        import logging.handlers
        import os
        import sys
        from dotenv import load_dotenv
        from typing import Optional
        
        
        class Settings:
            """
            Centralized configuration object holding constants and loaded environment variables.
            """
        
            def __init__(self):
                """Loads environment variables and sets configuration attributes."""
                logger = logging.getLogger("RAG_Chatbot_App")
                logger.info("Initializing application settings...")
                load_dotenv()
        
                self.OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")
                self.PINECONE_API_KEY: Optional[str] = os.getenv("PINECONE_API_KEY")
                self.PINECONE_INDEX_NAME: Optional[str] = os.getenv("PINECONE_INDEX_NAME")
                self.TAVILY_API_KEY: Optional[str] = os.getenv("TAVILY_API_KEY")
        
                required_keys = {
                    "OpenAI API Key": self.OPENAI_API_KEY,
                    "Pinecone API Key": self.PINECONE_API_KEY,
                    "Pinecone Index Name": self.PINECONE_INDEX_NAME,
                    "Tavily API Key": self.TAVILY_API_KEY,
                }
                missing_keys = [name for name, value in required_keys.items() if not value]
                if missing_keys:
                    error_msg = f"Error: Missing required environment variables: {', '.join(missing_keys)}"
                    logger.critical(error_msg.replace("Error: ", ""))
        
                    raise ValueError(error_msg)
                logger.info("Required environment variables loaded successfully.")
        
                self.CHUNK_SIZE: int = int(os.getenv("CHUNK_SIZE", "500"))
                self.CHUNK_OVERLAP: int = int(os.getenv("CHUNK_OVERLAP", "50"))
                self.TOP_K: int = int(os.getenv("TOP_K", "3"))
                self.TAVILY_MAX_RESULTS: int = int(os.getenv("TAVILY_MAX_RESULTS", "3"))
        
                self.PINECONE_BATCH_SIZE: int = int(os.getenv("PINECONE_BATCH_SIZE", "200"))
                self.PINECONE_MAX_REQUEST_BYTES: int = 2 * 1024 * 1024
                self.MAX_METADATA_FIELD_LENGTH: int = 1000
                self.MAX_TOTAL_METADATA_BYTES: int = 35 * 1024
                self.MAX_CHUNK_BYTES_WARNING: int = 1 * 1024 * 1024
        
                self.log_level_str: str = os.getenv("LOG_LEVEL", "INFO").upper()
                self.log_level: int = getattr(logging, self.log_level_str, logging.INFO)
                verbose_str = os.getenv("APP_VERBOSE", "false").lower()
                self.app_verbose: bool = verbose_str in ["true", "1", "yes", "on"]
                self.app_log_file: str = "app.log"
        
                logger.info("Application settings initialized.")
                logger.debug(
                    f"Settings loaded: ChunkSize={self.CHUNK_SIZE}, Overlap={self.CHUNK_OVERLAP}, BatchSize={self.PINECONE_BATCH_SIZE}, LogLevel={self.log_level_str}, Verbose={self.app_verbose}"
                )
        
        
        try:
            settings = Settings()
        except ValueError as e:
        
            logging.critical(f"Failed to initialize settings: {e}", exc_info=True)
        
            print(f"CRITICAL ERROR: Failed to initialize settings - {e}", file=sys.stderr)
            sys.exit(1)
        
        
        LOG_FORMAT = (
            "%(asctime)s - %(levelname)s - [%(name)s:%(filename)s:%(lineno)d] - %(message)s"
        )
        LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"
        
        
        def setup_logging():
            """Configures logging for the entire application based on environment variables."""
        
            verbose_str = os.getenv("APP_VERBOSE", "false").lower()
            APP_VERBOSE = verbose_str in ["true", "1", "yes", "on"]
            log_level_str = os.getenv("LOG_LEVEL", "INFO").upper()
            log_level = getattr(logging, log_level_str, logging.INFO)
            log_file = settings.app_log_file if "settings" in globals() else "app.log"
        
            app_logger = logging.getLogger("RAG_Chatbot_App")
            app_logger.setLevel(log_level)
            app_logger.propagate = False
        
            if app_logger.hasHandlers():
                for handler in app_logger.handlers[:]:
                    app_logger.removeHandler(handler)
                    handler.close()
        
            if APP_VERBOSE:
                # print(f"--- CONFIG: VERBOSE MODE ENABLED: Logging to '{log_file}' at level '{logging.getLevelName(log_level)}' ---", file=sys.stderr)
                formatter = logging.Formatter(LOG_FORMAT, datefmt=LOG_DATE_FORMAT)
                try:
                    file_handler = logging.FileHandler(log_file, mode="w", encoding="utf-8")
                    file_handler.setLevel(log_level)
                    file_handler.setFormatter(formatter)
                    app_logger.addHandler(file_handler)
        
                    app_logger.info(
                        f"VERBOSE File Logging Started (Overwrite Mode). Level: {log_level_str}"
                    )
                except Exception as e:
                    # print(f"--- FATAL ERROR: Failed to configure file logging for {log_file}: {e} ---", file=sys.stderr)
                    app_logger.addHandler(logging.NullHandler())
            else:
        
                app_logger.addHandler(logging.NullHandler())
        
            return app_logger
        --- END FILE CONTENT ---

    processing.py
        --- START FILE CONTENT ---
        import logging
        
        import streamlit as st
        
        from llm.components import (
            embed_and_upsert,
            load_and_split,
        )
        
        logger = logging.getLogger("RAG_Chatbot_App")
        
        
        def process_rag_sources(uploaded_files, urls):
            """Handles the processing of RAG data sources (files and URLs)."""
            logger.info("Starting RAG source processing workflow...")
            sources_to_process = []
            source_names = []
        
            if uploaded_files:
                for file in uploaded_files:
                    sources_to_process.append(file)
                    source_names.append(f"PDF: {file.name}")
                logger.info(f"Received {len(uploaded_files)} PDF files.")
        
            if urls:
                url_list = [url.strip() for url in urls.split("\n") if url.strip()]
                sources_to_process.extend(url_list)
                source_names.extend([f"URL: {url}" for url in url_list])
                logger.info(f"Received {len(url_list)} URLs.")
        
            if sources_to_process:
                logger.info(f"Starting processing for {len(source_names)} RAG sources.")
        
                with st.spinner("🔄 Processing RAG data sources..."):
                    try:
        
                        docs = load_and_split(sources_to_process)
                        # print(docs)
                        if docs:
                            logger.info("Initializing vectorstore for RAG...")
                            vectorstore = st.session_state.vectorstore
        
                            if vectorstore:
        
                                st.session_state.vectorstore = vectorstore
                                logger.info("RAG Vectorstore is ready.")
        
                                success = embed_and_upsert(docs, st.session_state.vectorstore)
                                if success:
        
                                    st.session_state.processed_sources = source_names
                                    logger.info(
                                        f"Successfully processed {len(source_names)} RAG sources."
                                    )
        
                                    st.success(
                                        f"✅ Successfully processed {len(source_names)} RAG sources!"
                                    )
                                    st.rerun()
                                else:
        
                                    logger.error("Error during RAG data embed/upsert.")
                                    st.error("❌ Embed/upsert RAG error.")
                            else:
        
                                logger.error("Could not connect to Pinecone Index for RAG.")
                                st.error("❌ Pinecone RAG connection error.")
                        else:
        
                            logger.warning(
                                "No content extracted from the provided RAG sources."
                            )
                            st.warning("⚠️ No RAG content extracted.")
                    except Exception as e:
        
                        logger.error(f"Error during RAG source processing: {e}", exc_info=True)
                        st.error(f"❌ Error processing RAG sources: {e}")
            else:
        
                logger.warning("No RAG sources provided for processing.")
                st.warning("⚠️ Please provide RAG data sources to process.")
        
        
        
        
        
        # # processing.py (Background Processing)
        
        # import logging
        # import threading 
        # from typing import List, Union
        
        # import streamlit as st
        # from streamlit.runtime.uploaded_file_manager import UploadedFile
        
        # from llm.components import (
        #     embed_and_upsert,
        #     get_pinecone_vectorstore,
        #     load_and_split,
        # )
        
        # # Use consistent application logger name
        # logger = logging.getLogger("RAG_Chatbot_App")
        
        # # --- Background Task Function ---
        # def _background_process_sources(sources_to_process: List[Union[str, UploadedFile]], source_names: List[str]):
        #     """
        #     This function runs in a separate thread to process RAG sources
        #     without blocking the main Streamlit app thread.
        #     It performs load, split, embed, and upsert to Pinecone.
        #     Updates session state upon completion or error.
        #     """
        #     thread_id = threading.get_ident()
        #     logger.info(f"[Thread-{thread_id}] Background RAG processing started for {len(sources_to_process)} sources.")
        
        #     try:
        #         # 1. Load and Split
        #         logger.info(f"[Thread-{thread_id}] Loading and splitting documents...")
        #         docs = load_and_split(sources_to_process)
        #         if not docs:
        #             logger.warning(f"[Thread-{thread_id}] No documents extracted after loading/splitting.")
        #             st.session_state.rag_processing_status = "error"
        #             st.session_state.rag_error_message = "No content extracted from sources."
        #             # Trigger rerun to update UI (safer than direct UI manipulation)
        #             # st.rerun() # Consider if rerun is needed here or rely on user interaction
        #             return # Exit thread
        
        #         logger.info(f"[Thread-{thread_id}] Successfully loaded and split into {len(docs)} chunks.")
        
        #         # 2. Initialize Vectorstore Connection (needed for upsert)
        #         index_name = st.session_state.get("PINECONE_INDEX_NAME")
        #         if not index_name:
        #             logger.error(f"[Thread-{thread_id}] PINECONE_INDEX_NAME missing. Cannot proceed with upsert.")
        #             st.session_state.rag_processing_status = "error"
        #             st.session_state.rag_error_message = "Configuration Error: Pinecone Index Name missing."
        #             # st.rerun()
        #             return
        
        #         logger.info(f"[Thread-{thread_id}] Connecting to Pinecone index '{index_name}' for upsert...")
        #         vectorstore = get_pinecone_vectorstore(index_name)
        #         if not vectorstore:
        #             logger.error(f"[Thread-{thread_id}] Failed to connect to Pinecone index '{index_name}'.")
        #             st.session_state.rag_processing_status = "error"
        #             st.session_state.rag_error_message = f"Failed to connect to Pinecone index '{index_name}'."
        #             # st.rerun()
        #             return
        
        #         # 3. Embed and Upsert
        #         logger.info(f"[Thread-{thread_id}] Starting embed and upsert to Pinecone...")
        #         success = embed_and_upsert(docs, vectorstore) # This function now contains robust cleaning
        
        #         if success:
        #             logger.info(f"[Thread-{thread_id}] Successfully processed and upserted {len(source_names)} sources to Pinecone.")
        #             # Update state to indicate completion and store the ready vectorstore
        #             st.session_state.rag_processing_status = "completed"
        #             st.session_state.processed_sources = source_names
        #             st.session_state.vectorstore = vectorstore # Store the ready vectorstore connection
        #             st.session_state.rag_error_message = None # Clear any previous error
        #         else:
        #             logger.error(f"[Thread-{thread_id}] Embed/upsert process failed.")
        #             st.session_state.rag_processing_status = "error"
        #             st.session_state.rag_error_message = "Failed to embed or save data to the vector database."
        #             st.session_state.vectorstore = None # Ensure vectorstore is None on error
        
        #     except Exception as e:
        #         logger.error(f"[Thread-{thread_id}] Unhandled error during background processing: {e}", exc_info=True)
        #         st.session_state.rag_processing_status = "error"
        #         st.session_state.rag_error_message = f"An unexpected error occurred: {e}"
        #         st.session_state.vectorstore = None # Ensure vectorstore is None on error
        #     finally:
        #         logger.info(f"[Thread-{thread_id}] Background RAG processing thread finished.")
        #         # Trigger a rerun for the main thread to pick up the state changes
        #         # This is generally safe as it just schedules a rerun, not direct UI manipulation
        #         try:
        #              st.rerun()
        #         except Exception as rerun_err:
        #              # This might happen if called at an awkward time during Streamlit shutdown
        #              logger.warning(f"[Thread-{thread_id}] Could not trigger rerun after background task: {rerun_err}")
        
        
        # # --- Main Function Called by UI ---
        # def process_rag_sources(uploaded_files: List[UploadedFile], urls: str):
        #     """
        #     Handles the request to process RAG sources.
        #     Starts the actual processing in a background thread.
        #     Updates the UI immediately to indicate processing has started.
        #     """
        #     logger.info("Processing RAG sources request received.")
        #     sources_to_process = []
        #     source_names = []
        
        #     # Collect sources (same logic as before)
        #     if uploaded_files:
        #         for file in uploaded_files: sources_to_process.append(file); source_names.append(f"PDF: {file.name}")
        #         logger.info(f"Received {len(uploaded_files)} PDF files.")
        #     if urls:
        #         url_list = [url.strip() for url in urls.split("\n") if url.strip()]; sources_to_process.extend(url_list); source_names.extend([f"URL: {url}" for url in url_list])
        #         logger.info(f"Received {len(url_list)} URLs.")
        
        #     if sources_to_process:
        #         # --- Start Background Thread ---
        #         logger.info(f"Starting background thread to process {len(source_names)} sources.")
        #         # Set status to running *before* starting thread
        #         st.session_state.rag_processing_status = "running"
        #         st.session_state.rag_error_message = None # Clear previous errors
        #         st.session_state.processed_sources = [] # Clear previous sources list while processing
        
        #         # Create and start the thread
        #         thread = threading.Thread(
        #             target=_background_process_sources,
        #             args=(sources_to_process, source_names),
        #             daemon=True # Allows main program to exit even if thread is running (optional)
        #         )
        #         thread.start()
        
        #         # --- Update UI Immediately ---
        #         # Don't wait for the thread here.
        #         # Show a message indicating background processing.
        #         # A rerun is usually triggered automatically by the button click,
        #         # or we can force one if needed to ensure the spinner/message shows.
        #         logger.info("Background thread started. Main thread continues.")
        #         # st.rerun() # Force rerun to update UI immediately if button click doesn't suffice
        
        #     else:
        #         logger.warning("No RAG sources provided for processing.")
        #         st.warning("⚠️ Please provide RAG data sources to process.") # Show warning in UI
        
        --- END FILE CONTENT ---

    state_manager.py
        --- START FILE CONTENT ---
        # state_manager.py (Change 1 - Initialize Pinecone VS)
        
        import logging
        import streamlit as st
        
        # Adjust the import based on your project structure
        from llm.graph_logic import build_graph
        from llm.components import get_pinecone_vectorstore
        
        # Use consistent application logger name
        logger = logging.getLogger("RAG_Chatbot_App")
        
        
        def initialize_session_state():
            """Initializes necessary values in Streamlit's session state,
               including the persistent Pinecone vectorstore connection."""
        
            # Initialize basic states if they don't exist
            if "messages" not in st.session_state:
                st.session_state.messages = []
                logger.debug("Initialized 'messages' in session state.")
            if "processed_sources" not in st.session_state:
                st.session_state.processed_sources = []
                logger.debug("Initialized 'processed_sources' in session state.")
            if "mode_internal" not in st.session_state:
                st.session_state.mode_internal = "Web Search" # Default mode
                logger.debug("Initialized default chatbot mode (internal): Web Search")
        
            # --- Initialize Pinecone Vectorstore Connection ---
            if "vectorstore" not in st.session_state:
                logger.info("Attempting to initialize Pinecone vectorstore connection...")
                index_name = st.session_state.get("PINECONE_INDEX_NAME") # Get index name loaded earlier
                if index_name:
                    # Call the function to get the vectorstore object
                    vs = get_pinecone_vectorstore(index_name)
                    if vs:
                        st.session_state.vectorstore = vs # Store the connection object
                        logger.info("Pinecone vectorstore initialized and stored in session state.")
                    else:
                        st.session_state.vectorstore = None # Explicitly set to None if connection failed
                        logger.error("Failed to initialize Pinecone vectorstore during session state init.")
                        # Optionally show a warning in the UI, but app can continue in Web Search mode
                        # st.warning("Could not connect to the RAG vector database. RAG mode may not function correctly.")
                else:
                    st.session_state.vectorstore = None # Set to None if index name is missing
                    logger.error("Cannot initialize Pinecone vectorstore: PINECONE_INDEX_NAME missing from session state.")
                    # st.error("Configuration Error: Cannot initialize RAG database - Index Name missing.")
            # ---
        
            # Initialize Graph (needs to happen after other states are potentially set)
            if "graph" not in st.session_state:
                logger.info("Compiling graph for the first time...")
                try:
                    st.session_state.graph = build_graph()
                    logger.info("Compiled and saved graph to session state.")
                except Exception as e:
                    logger.error(f"Critical error compiling graph: {e}", exc_info=True)
                    st.error(f"Cannot initialize application due to graph compilation error: {e}")
                    st.stop()
        
        --- END FILE CONTENT ---

    ui_chat.py
        --- START FILE CONTENT ---
        import streamlit as st
        
        
        def display_chat_history():
            """Displays the chat history from session state."""
        
            for message in st.session_state.get("messages", []):
        
                avatar = "👤" if message.get("role") == "user" else "🤖"
        
                with st.chat_message(message.get("role", "assistant"), avatar=avatar):
        
                    st.markdown(message.get("content", ""))
        
        
        def display_ai_message_elements():
            """Creates and returns placeholders for the AI response and status list."""
            response_placeholder = st.empty()
        
            with st.expander("⚙️ Detailed Processing Steps:", expanded=False):
                status_list_placeholder = st.empty()
        
            return response_placeholder, None, status_list_placeholder
        --- END FILE CONTENT ---

    ui_sidebar.py
        --- START FILE CONTENT ---
        import logging
        
        import streamlit as st
        
        logger = logging.getLogger("RAG_Chatbot_App")
        
        
        def update_chatbot_mode():
            """Callback function called when the user changes the chatbot mode."""
        
            selected_display_mode = st.session_state.chatbot_mode_selector
            logger.debug(
                f"Callback update_chatbot_mode - Radio value selected: {selected_display_mode}"
            )
        
            if selected_display_mode == "Web Search (Tavily)":
                st.session_state.mode_internal = "Web Search"
            else:
                st.session_state.mode_internal = "RAG"
            logger.info(
                f"Callback: Chatbot mode (internal) updated to: {st.session_state.mode_internal}"
            )
        
        
        def display_sidebar():
            """Displays all sidebar content and returns necessary widget values."""
            with st.sidebar:
                st.header("Chatbot Configuration")
        
                mode_options = ["Web Search (Tavily)", "RAG (Uploaded Documents)"]
        
                current_mode_index = (
                    0
                    if st.session_state.get("mode_internal", "Web Search") == "Web Search"
                    else 1
                )
                st.radio(
                    "Select operating mode:",
                    options=mode_options,
                    key="chatbot_mode_selector",
                    index=current_mode_index,
                    on_change=update_chatbot_mode,
                )
        
                st.markdown("---")
                st.header("RAG Data Sources")
                st.caption("Process sources if you want to use RAG mode.")
                uploaded_files = st.file_uploader(
                    "Upload PDF files",
                    type=["pdf"],
                    accept_multiple_files=True,
                    key="pdf_uploader",
                )
                urls = st.text_area("Or enter URLs (one per line)", key="url_input")
                process_button = st.button("Process RAG Sources", key="process_sources_btn")
        
                st.subheader("Processed Sources (for RAG):")
                if st.session_state.get("processed_sources"):
                    for src in st.session_state.processed_sources:
                        st.write(f"- {src}")
                else:
                    st.write("No sources processed yet.")
        
            return uploaded_files, urls, process_button
        --- END FILE CONTENT ---

[.streamlit/]
    config.toml
        --- START FILE CONTENT ---
        [theme]
        base="dark"
        --- END FILE CONTENT ---

================================================================================
# End of Project Structure and Content
